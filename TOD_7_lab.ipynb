{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5QwFjKatO6z"
      },
      "source": [
        "# Введение в обработку текста на естественном языке"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvq3EZWCtO62"
      },
      "source": [
        "## Задачи для совместного разбора"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1mp6iBrtO63"
      },
      "source": [
        "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOrF5fM8tO64"
      },
      "outputs": [],
      "source": [
        "text = '''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "# читаем слова из файла в список words\n",
        "with open('litw-win.txt', encoding='cp1251') as f:\n",
        "    words = f.read().split()\n",
        "\n",
        "# исправляем опечатки в предложении\n",
        "sentence = 'с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'\n",
        "corrected_sentence = []\n",
        "for word in sentence.split():\n",
        "    if word in words:\n",
        "        corrected_sentence.append(word)\n",
        "    else:\n",
        "        closest_word = difflib.get_close_matches(word, words, n=1)\n",
        "        corrected_sentence.append(closest_word[0])\n",
        "        \n",
        "print(*corrected_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll8lMhWg6jtP",
        "outputId": "3adffdf7-8c35-4606-b658-fccc0c52fcbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "с величайшим усилием выбравшись из потока убегающих людей кутузов со свитой уменьшившейся вдвое поехал на звуки выстрелов русских орудий\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZL0QfdntO64"
      },
      "source": [
        "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# текст из прошлого задания\n",
        "text = 'с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'\n",
        "\n",
        "# разбиваем текст на слова\n",
        "words = nltk.word_tokenize(text, language='russian')\n",
        "\n",
        "# стемминг слов\n",
        "stemmer = SnowballStemmer('russian')\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# лемматизация слов\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print('Слова: ', words)\n",
        "print('Стемминг: ', stemmed_words)\n",
        "print('Лемматизация: ', lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEPtvkdEAOjN",
        "outputId": "ab8c25b7-965f-4c05-ee42-14c74bbed938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Слова:  ['с', 'велечайшим', 'усилием', 'выбравшись', 'из', 'потока', 'убегающих', 'людей', 'Кутузов', 'со', 'свитой', 'уменьшевшейся', 'вдвое', 'поехал', 'на', 'звуки', 'выстрелов', 'русских', 'орудий']\n",
            "Стемминг:  ['с', 'велечайш', 'усил', 'выбра', 'из', 'поток', 'убега', 'люд', 'кутуз', 'со', 'свит', 'уменьшевш', 'вдво', 'поеха', 'на', 'звук', 'выстрел', 'русск', 'оруд']\n",
            "Лемматизация:  ['с', 'велечайшим', 'усилием', 'выбравшись', 'из', 'потока', 'убегающих', 'людей', 'Кутузов', 'со', 'свитой', 'уменьшевшейся', 'вдвое', 'поехал', 'на', 'звуки', 'выстрелов', 'русских', 'орудий']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqt9Q6DhtO64"
      },
      "source": [
        "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sentence = ['с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий']\n",
        "\n",
        "# создаем объект CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# преобразуем предложения в векторы\n",
        "vectorized_sentence = vectorizer.fit_transform(sentence)\n",
        "\n",
        "# выводим словарь (т.е. уникальные слова) и их индексы\n",
        "print('Словарь: ')\n",
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiGOaQFdB_Bu",
        "outputId": "b4e7b897-0915-46b8-ecdb-8466c9d623ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Словарь: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'велечайшим': 1,\n",
              " 'усилием': 17,\n",
              " 'выбравшись': 2,\n",
              " 'из': 5,\n",
              " 'потока': 11,\n",
              " 'убегающих': 15,\n",
              " 'людей': 7,\n",
              " 'кутузов': 6,\n",
              " 'со': 14,\n",
              " 'свитой': 13,\n",
              " 'уменьшевшейся': 16,\n",
              " 'вдвое': 0,\n",
              " 'поехал': 10,\n",
              " 'на': 8,\n",
              " 'звуки': 4,\n",
              " 'выстрелов': 3,\n",
              " 'русских': 12,\n",
              " 'орудий': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl8g0fKCtO64"
      },
      "source": [
        "## Лабораторная работа 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seAOgvshtO64"
      },
      "source": [
        "### Расстояние редактирования"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ghJvV8vtO65"
      },
      "source": [
        "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "\n",
        "# загружаем данные из файла preprocessed_descriptions.csv\n",
        "with open('preprocessed_descriptions.csv', newline='', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=' ', quotechar='|', )\n",
        "    # пропускаем заголовки\n",
        "    \n",
        "    # собираем все слова из всех описаний в один список\n",
        "    words = []\n",
        "    for row in reader:\n",
        "        tokens = nltk.word_tokenize(row[0])\n",
        "        words.extend(tokens)\n",
        "\n",
        "# создаем множество уникальных слов\n",
        "unique_words = set(words)\n",
        "\n",
        "# выводим количество уникальных слов и несколько примеров\n",
        "print('Количество уникальных слов:', len(unique_words))\n",
        "print('Примеры уникальных слов:', list(unique_words)[:10])"
      ],
      "metadata": {
        "id": "cB8luEMeQFmv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50affbf2-97ba-4806-8d0a-f6a50eb8a113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество уникальных слов: 35011\n",
            "Примеры уникальных слов: ['10827', 'ramen', '15579', '28021', '13968', 'brown', '29754', '11376', '15805', 'humble']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBhld1e0tO65"
      },
      "source": [
        "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from nltk.metrics import distance\n",
        "\n",
        "# выбираем 5 пар случайных слов\n",
        "pairs = random.sample(unique_words, k=10)\n",
        "\n",
        "# выводим расстояние редактирования между каждой парой слов\n",
        "for word1, word2 in zip(pairs[::2], pairs[1::2]):\n",
        "    print(f\"Расстояние редактирования между '{word1}' и '{word2}': {distance.edit_distance(word1, word2)}\")"
      ],
      "metadata": {
        "id": "r_C2WWvBR2Lu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a600cf-ea04-4f21-d7ff-c4e25669fd6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Расстояние редактирования между '9293' и 'ribs': 4\n",
            "Расстояние редактирования между '13897' и '8226': 5\n",
            "Расстояние редактирования между '27864' и '303': 5\n",
            "Расстояние редактирования между '28504' и '18439': 4\n",
            "Расстояние редактирования между '14736' и '6706': 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-9c70e2ce37bf>:5: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  pairs = random.sample(unique_words, k=10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bs94pg9tO65"
      },
      "source": [
        "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "def closest_words(word, words):\n",
        "    return min(words, key=lambda x: edit_distance(x, word))\n",
        "\n",
        "word = 'appel'\n",
        "closest_word = closest_words(word, words)\n",
        "print(f\"Ближайшее слово к '{word}': {closest_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJMJTioaVGAS",
        "outputId": "e1019cd7-97ad-4ec0-d317-058389cbbbc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ближайшее слово к 'appel': angel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oiiy1IWHtO65"
      },
      "source": [
        "### Стемминг, лемматизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VmZPO6MtO65"
      },
      "source": [
        "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
        "    * word\n",
        "    * stemmed_word \n",
        "    * normalized_word \n",
        "\n",
        "Столбец `word` укажите в качестве индекса. \n",
        "\n",
        "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = SnowballStemmer('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Стемминг и лемматизация слов\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "normalized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "# Создание DataFrame\n",
        "df = pd.DataFrame({'word': words, 'stemmed_word': stemmed_words, 'normalized_word': normalized_words})\n",
        "df = df.set_index('word')\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "6EjHonpCX7v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "994c86a9-1aa0-4e9c-b8f4-087166026162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       stemmed_word normalized_word\n",
            "word                               \n",
            "0                 0               0\n",
            ",                 ,               ,\n",
            "george        georg          george\n",
            "1                 1               1\n",
            ",                 ,               ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlvhZ5LLtO66"
      },
      "source": [
        "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "CYsiWo86Ygpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка описаний рецептов\n",
        "data = pd.read_csv('preprocessed_descriptions.csv', delimiter=' ',)\n",
        "\n",
        "# Удаление стоп-слов из описаний\n",
        "stop_words = set(stopwords.words('english'))\n",
        "data['description'] = data['description'].apply(lambda x: [word for word in word_tokenize(x) if word.lower() not in stop_words])\n",
        "\n",
        "# Определение доли стоп-слов\n",
        "total_words = data['description'].apply(lambda x: len(x)).sum()\n",
        "stop_words_count = sum(data['description'].apply(lambda x: x.count(w) for w in stop_words))\n",
        "stop_words_fraction = stop_words_count / total_words\n",
        "print(f\"Доля стоп-слов: {stop_words_fraction:.3f}\")\n",
        "\n",
        "# Топ-10 слов до удаления стоп-слов\n",
        "all_words = [word for desc in data['description'] for word in desc]\n",
        "top_10_words_before = pd.Series(all_words).value_counts().head(10)\n",
        "print(\"Топ-10 слов до удаления стоп-слов:\\n\", top_10_words_before)\n",
        "\n",
        "# Топ-10 слов после удаления стоп-слов\n",
        "all_words = [word for desc in data['description'] for word in desc]\n",
        "top_10_words_after = pd.Series(all_words).value_counts().head(10)\n",
        "print(\"Топ-10 слов после удаления стоп-слов:\\n\", top_10_words_after)"
      ],
      "metadata": {
        "id": "LxKe_5n0TNj2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}